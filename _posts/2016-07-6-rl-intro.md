---
layout:     post
title:      Reinforcement learning in robotics
date:       2016-07-06 
summary:    What's reinforcement lerarning? how's reinfocement learning relevant for robotics? how do we apply reinforcement learning in real problems?. All together discussed while introducing a comprehensive example of Q-learning, one of the easiests approaches to RL.
categories: robots, ai, deep learning, rl, reinforcement learning
mathjax: true
---


<p style="border: 2px solid #000000; padding: 10px; background-color: #E5E5E5; color: black; font-weight: light;">
 Reinforcement Learning (RL) is a subfield of Machine Learning where an agent learns by interacting with its environment, observing the results of these interactions and receiving a reward (positive or negative) accordingly. This way of learning mimics the fundamental way in which we humans (and animals alike) learn.
</p>

We humans, have a direct sensori-motor connection to our environment that can perform actions and witness the results of these actions on the environment itself. The idea is commonly known as *"cause and effect"*, and this undoubtedly is the key to building up knowledge of our environment throughout our lifetime. 
The content below will get into the following topics:

- [How's Reinforcement Learning being used today?](#rluses)
- [Why is Reinforcement Learning relevant for robots?](#rlrobotics)
- [A comprehensive approach to Reinforcement Learning?](#qlearning)
  - [Background](#background)
  - [The *World* and *Cat* player implementations](#world)
  - [The *Mouse* player](#mouse)
    - [Q-learning implementation](#q-learning)
  - [Results](#results)
  - [Reproduce it yourself](#reproduce)

<div id='rluses'/>
### How's Reinforcement Learning being used today?

A variety of different problems can be solved using Reinforcement Learning. Because RL agents can learn without expert supervision, the type of problems that are best suited to RL are complex problems where there appears to be no obvious or easily programmable solution. Two of the most popular ones are:

- **Game playing** - determining the best move to make in a game often depends on a number of different factors, hence the number of possible states that can exist in a particular game is usually very large. To cover this many states using a standard rule based approach would mean specifying an also large number of hard coded rules. RL cuts out the need to manually specify rules, agents learn simply by playing the game. For two player games such as backgammon, agents can be trained by playing against other human players or even other RL agents.

- **Control problems** - such as elevator scheduling. Again, it is not obvious what strategies would provide the best, most timely elevator service. For control problems such as this, RL agents can be left to learn in a simulated environment and eventually they will come up with good controlling policies. Some advantages of using RL for control problems is that an agent can be retrained easily to adapt to environment changes, and trained continuously while the system is online, improving performance all the time.

A nice and relatively recent (at the time of writing) example of reinforcement learning is presented at DeepMind's paper [Human-level control through deep reinforcement learning](https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf).

<div id='rlrobotics'/>
### Why is Reinforcement Learning relevant for robots?

As J. Kober, J. Andrew (Drew) Bagnell, and J. Peters point out in *Reinforcement Learning in Robotics: A Survey*:

>Reinforcement learning offers to robotics a framework
and set of tools for the design of sophisticated
and hard-to-engineer behaviors.

which tend to be most of the existing ones in the real world. As a rule of thumb, someone can probably think that those tasks that are complex for a human could probably be easily done by a robot while things that we humans do easily and with little effort, tend to be highly complex for a robot. This leads many to think that robots excel at solving complexity but perform poorly on trivial tasks (from a human perspective). In other words:

>What's complex for a human, robots can do easily and viceversa - VÃ­ctor Mayoral Vilches

Let's take a simple example to illustrate this claim. Image we have a robot manipulator with three joints on top of a table repeatedly performing some task. Traditionally, robotic engineers in charge of the specific task will either design the whole application or use existing tools (produced by the manufacturer) to *program* such application. Regardless of the tools and complexity level, at some point someone should've derived its inverse kinematics accounting for possible errors within each motor/joint, included sensing to create a closed loop that enhances the accuracy of the model, designed the overall control scheme, programmed *calibration routines*, ... all to get a robot that produces deterministic movements in a **controlled environement**.

But the fact is that **(real) environments are not controllable** and robots nowadays pay the bill (someone that has ever produced robot demonstrations should know what i mean).

>Problems in robotics are often best represented with
high-dimensional, continuous states and actions (note
that the 10-30 dimensional continuous actions common
in robot reinforcement learning are considered large
(Powell, 2012)). In robotics, it is often unrealistic to
assume that the true state is completely observable
and noise-free. 

Coming back to J. Kober et al.'s paper:

>Reinforcement learning (RL) enables a robot to
autonomously discover an optimal behavior through
trial-and-error interactions with its environment. Instead
of explicitly detailing the solution to a problem,
in reinforcement learning the designer of a control task
provides feedback in terms of a scalar objective function
that measures the one-step performance of the
robot.

Which actually makes a lot of sense!. Think of how we learn about speficic tasks. Let's take me shooting a basketball:

1. I get myself behind the 3 point line and get ready for a shot (it's relevant to note it should be behind the 3 point line, otherwise i would totally make it every time)

2. At this point, my consciousness has no whatsoever information about the exact distance to the basket, neither the strength I should use to use to make the shot so my brain produces *an estimate* based on the model that I have (built upon years of trial an error shots).

3. With this estimate, I produce a shot. Let's assume I miss the shot, which I notice through my eyes (sensors). Again, the information perceived through my eyes is not accurate thereby what I pass to my brain is not: "*I missed the shot by 5.45 cm to the right*" but more like "*The shot was slightly too much to the right and i missed*". This information updates the model in my brain which receives a *negative reward*.
We could get ourselves discussing about why did my estimate failed. Was it because the model is wrong regardless of the fact that I've made hundreds of 3-pointers before with that exact model? Was it because of the wind (playing outdoors generally)? or was it because i didn't eat properly that morning?. It could easily be all of those or none, but the fact is that many of those aspects can't really be controlled be me. So i proceed *iterating*:

4. With the updated model, i make another shot which in case it fails drives me to step 2) but if I make it, I proceed to step 5).

5. Making a shot means that my model did a good job so my brain strenghthens those links that produced a proper shot by giving them a *positive reward*.

For robotics to grow exponentially, we need to be able to tackle complex problems and the trial-error approach that RL represents seems the right way.

<div id='qlearning'/>
### A comprehensive approach to Reinforcement Learning: Q-learning

The following content will introduce RL via Q-learning using the cat-mouse-cheese example. 

<div id='background'/>
#### Background

<p style="border: 2px solid #000000; padding: 10px; background-color: #E5E5E5; color: black; font-weight: light;">
Value Functions are state-action pair functions that estimate how good a particular action will be in a given state, or what the return for that action is expected to be.
</p>

Q-Learning is an *off-policy* (can update the estimated value functions using hypothetical actions, those which have not actually been tried) algorithm for *temporal difference learning* ( method to estimate value functions). It can be proven that given sufficient training, the Q-learning converges with probability 1 to a close approximation of the action-value function for an arbitrary target policy. Q-Learning learns the optimal policy even when actions are selected according to a more exploratory or even random policy. Q-learning can be implemented as follows:

```
Initialize Q(s,a) arbitrarily
Repeat (for each generation):
	Initialize state s
	While (s is not a terminal state):		
		Choose a from s using policy derived from Q
		Take action a, observe r, s'
		Q(s,a) += alpha * (r + gamma * max,Q(s') - Q(s,a))
		s = s'
```

where:

 - **s**: is the previous state
 - **a**: is the previous action
 - **Q()**: is the Q-learning algorithm
 - **s'**: is the current state
 - **alpha**: is the the learning rate, set generally between 0 and 1. Setting it to 0 means that the Q-values are never updated, thereby nothing is learned. Setting  alpha to a high value such as 0.9 means that learning can occur quickly.
 - **gamma**: is the discount factor, also set between 0 and 1. This models the fact that future rewards are worth less than immediate rewards.
 - **max,**: is the the maximum reward that is attainable in the state following the current one (the reward for taking the optimal action thereafter).


From a math perspective, the algoritm updates its internal state following:

$$Q(s,a) += \alpha \cdot [r + \gamma \cdot max_\alpha Q(s') - Q(s,a)]$$

<!-- $$a^2 + b^2 = c^2$$

$$ \mathsf{Data = PCs} \times \mathsf{Loadings} $$

\\[ \mathbf{X} = \mathbf{Z} \mathbf{P^\mathsf{T}} \\]
-->

The algorithm can be interpreted as:

1. Initialize the Q-values table, Q(s, a).
2. Observe the current state, s.
3. Choose an action, a, for that state based on the selection policy.
3. Take the action, and observe the reward, r, as well as the new state, s'.
4. Update the Q-value for the state using the observed reward and the maximum reward possible for the next state.
5. Set the state to the new state, and repeat the process until a terminal state is reached.



<div id='world'/>
#### The *World* and *Cat* player implementations

The implementations of the discrete 2D world (including agents, cells and other abstractions) as well as the cat and mouse players is performed in the `cellular.py` file. The world is generated from a `.txt`file. In particular, I'm using the `worlds/waco.txt`:

```
(waco world)

XXXXXXXXXXXXXX
X            X
X XXX X   XX X
X  X  XX XXX X
X XX      X  X
X    X  X    X
X X XXX X XXXX
X X  X  X    X
X XX   XXX  XX
X    X       X
XXXXXXXXXXXXXX

```

The *Cat* player class inherit from `cellular.Agent` and its implementation is set to follow the *Mouse* player:

```python

    def goTowards(self, target):
        if self.cell == target:
            return
        best = None
        for n in self.cell.neighbours:
            if n == target:
                best = target
                break
            dist = (n.x - target.x) ** 2 + (n.y - target.y) ** 2
            if best is None or bestDist > dist:
                best = n
                bestDist = dist
        if best is not None:
            if getattr(best, 'wall', False):
                return
            self.cell = best

```
The *Cat* player calculates the quadratic distance (`bestDist`)  among its neighbours and moves itself (`self.cell = best`) to that cell.

```python
class Cat(cellular.Agent):
    cell = None
    score = 0
    colour = 'orange'

    def update(self):
        cell = self.cell
        if cell != mouse.cell:
            self.goTowards(mouse.cell)
            while cell == self.cell:
                self.goInDirection(random.randrange(directions))

```
Overall, the *Cat* pursues the *Mouse* through the `goTowards` method by calculating the quadratic distance. Whenever it bumps to the wall, it takes a random action.

<div id='mouse'/>
#### The *Mouse* player

The *Mouse* player contains the following attributes:

```python
class Mouse(cellular.Agent):
    colour = 'gray'

    def __init__(self):
        self.ai = None
        self.ai = qlearn.QLearn(actions=range(directions),
                                alpha=0.1, gamma=0.9, epsilon=0.1)
        self.eaten = 0
        self.fed = 0
        self.lastState = None
        self.lastAction = None

```

The `eaten` and `fed` attributes store the performance of the player while the `lastState` and `lastAction` ones help the *Mouse* player store information about its previous states (later used for learning).

The `ai` attribute stores the Q-learning implementation which is initialized with the following parameters:

- **directions**: There're different possible directions/actions implemented at the `getPointInDirection` method of the *World* class:

```python
    def getPointInDirection(self, x, y, dir):
        if self.directions == 8:
            dx, dy = [(0, -1), (1, -1), (
                1, 0), (1, 1), (0, 1), (-1, 1), (-1, 0), (-1, -1)][dir]
        elif self.directions == 4:
            dx, dy = [(0, -1), (1, 0), (0, 1), (-1, 0)][dir]
        elif self.directions == 6:
            if y % 2 == 0:
                dx, dy = [(1, 0), (0, 1), (-1, 1), (-1, 0),
                          (-1, -1), (0, -1)][dir]
            else:
                dx, dy = [(1, 0), (1, 1), (0, 1), (-1, 0),
                          (0, -1), (1, -1)][dir]


```
In general, this implementaiton will be used in **8 directions**, thererby `(0, -1), (1, -1), (1, 0), (1, 1), (0, 1), (-1, 1), (-1, 0), (-1, -1)]`.

- **alpha**: which is the q-learning discount constant, set to `0.1`.
- **gamma**: the q-learning discount factor, set to `0.9`.
- **epsilon**: an exploration constant to randomize decisions, set to `0.1`.

The *Mouse* player calculates the next state using the `calcState()` method implemented as follows:

```python

    def calcState(self):
        def cellvalue(cell):
            if cat.cell is not None and (cell.x == cat.cell.x and
                                         cell.y == cat.cell.y):
                return 3
            elif cheese.cell is not None and (cell.x == cheese.cell.x and
                                              cell.y == cheese.cell.y):
                return 2
            else:
                return 1 if cell.wall else 0

        return tuple([cellvalue(self.world.getWrappedCell(self.cell.x + j, self.cell.y + i))
                      for i,j in lookcells])

```

This, in a nutshell, returns a tupple of the values of the cells surrounding the current *Mouse* as follows:

- `3`: if the *Cat* is in that cell
- `2`: if the *Cheese* is in that cell
- `1`: if the that cell is a wall
- `0`: otherwise

The lookup is performed according to the `lookdist` variable that in this implementation uses a value of `2` (in other words, the mouse can "see" up to two cells ahead in every direction).

To finish up reviewing the *Mouse* implementation, let's look at how the Q-learning is implemented:

<div id='q-learning'/>
##### Q-learning implementation
Let's look at the `update` method of the *Mouse* player:

```python
    def update(self):
        # calculate the state of the surrounding cells
        state = self.calcState()
        # asign a reward of -1 by default
        reward = -1

        # Update the Q-value
        if self.cell == cat.cell:
            self.eaten += 1
            reward = -100
            if self.lastState is not None:
                self.ai.learn(self.lastState, self.lastAction, reward, state)
            self.lastState = None

            self.cell = pickRandomLocation()
            return

        if self.cell == cheese.cell:
            self.fed += 1
            reward = 50
            cheese.cell = pickRandomLocation()

        if self.lastState is not None:
            self.ai.learn(self.lastState, self.lastAction, reward, state)

        # Choose a new action and execute it
        state = self.calcState()
        action = self.ai.chooseAction(state)
        self.lastState = state
        self.lastAction = action

        self.goInDirection(action)
```

Code has been commented so that its understanding is simplified. The implementation matches the pseudo-code presented in the [Background](#background) section above (note that for the sake of the implementation, the actions in the `Python` implementation have been reordered). 

Rewards are given with these terms:

- `-100`: if the *Cat* player eats the *Mouse*
- `50`: if the *Mouse* player eats the cheese
- `-1`: otherwise

The learning algorithm records every state/action/reward combination in a dictionary containing a (state, action) tuple in the key and the reward as the value of each member.

Note that the amount of elements saved in the dictionary for this simplified 2D environment is considerable after a few generations. To get some insight about this fact, consider the following numbers:

- After **10 000** generations:
  	- 2430 elements (state/action/reward combinations) learned  	
  	- Bytes: 196888 (192 KB)

- After **100 000** generations:
	- 5631 elements (state/action/reward combinations) learned
	- Bytes: 786712 (768 KB)

- After **600 000** generations:
	- 9514 elements (state/action/reward combinations) learned
	- Bytes: 786712 (768 KB)

- After **1 000 000** generations:
	- 10440 elements (state/action/reward combinations) learned
	- Bytes: 786712 (768 KB)

Given the results showed above, one can observe that for some reason, Python `sys.getsizeof` function seems to be upper bounded by 786712 (768 KB). We can't provide accurate data but given the results showed, one can conclude that the elements generated after **1 million generations should require something close to 10 MB in memory** for this 2D simplified world.

*Edit: The actual reason why the size of the elements stored get constant (no matter how many iterations we go though) is because the world saturates (all the different possibilities have already been stored in the table). By using a bigger world one sees that the conclusions in the paragraph above are indeed met*.

<div id='results'/>
#### Results
Below we present a *mouse player* after **15 000 generations** of reinforcement learning:

![](https://github.com/vmayoral/basic_reinforcement_learning/raw/master/img/rl_qlearning_1.gif)

and now we present the same player after **150 000 generations** of reinforcement learning:

![](https://github.com/vmayoral/basic_reinforcement_learning/raw/master/img/rl_qlearning_2.gif)

<div id='reproduce'/>
#### Reproduce it yourself

```bash
git clone https://github.com/vmayoral/basic_reinforcement_learning
cd basic_reinforcement_learning
python tutorial1/egoMouseLook.py
```


### Resources
- Chris Watkins, Learning from Delayed Rewards, Cambridge, 1989 ([thesis](http://www.cs.rhul.ac.uk/home/chrisw/new_thesis.pdf))
- Awesome Reinforcement Learning repository, [https://github.com/aikorea/awesome-rl](https://github.com/aikorea/awesome-rl)
- Reinforcement learning CS9417ML, School of Computer Science & Engineering, UNSW Sydney, [http://www.cse.unsw.edu.au/~cs9417ml/RL1/index.html](http://www.cse.unsw.edu.au/~cs9417ml/RL1/index.html)
- Reinforcement learning blog posts, [https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/](https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/)
- J. Kober, J. Andrew (Drew) Bagnell, and J. Peters, "Reinforcement Learning in Robotics: A Survey," International Journal of Robotics Research, July, 2013. ([pdf](http://www.ias.tu-darmstadt.de/uploads/Publications/Kober_IJRR_2013.pdf))
