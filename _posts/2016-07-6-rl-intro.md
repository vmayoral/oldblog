---
layout:     post
title:      Reinforcement learning in robotics
date:       2016-07-06 
summary:    What's reinforcement lerarning? how's reinfocement learning relevant for robotics? how do we apply reinforcement learning in real problems?. All together discussed while introducing a comprehensive example of Q-learning, one of the easiests approaches to RL.
categories: robots, ai, deep learning, rl, reinforcement learning
mathjax: true
---


<p style="border: 2px solid #000000; padding: 10px; background-color: #E5E5E5; color: black; font-weight: light;">
 Reinforcement Learning (RL) is a subfield of Machine Learning where an agent learns by interacting with its environment, observing the results of these interactions and receiving a reward (positive or negative) accordingly. This way of learning mimics the fundamental way in which we humans (and animals alike) learn.
</p>

We humans, have a direct sensori-motor connection to our environment that can perform actions and witness the results of these actions on the environment itself. The idea is commonly known as *"cause and effect"*, and this undoubtedly is the key to building up knowledge of our environment throughout our lifetime. 
The content below will get into the following topics:

- [How's Reinforcement Learning being used today?](#rluses)
- [Why is Reinforcement Learning relevant for robots?](#rlrobotics)
- [Robot Reinforcement Learning](#rrltecniques)
  - [Function approximation](#function)
- [Challenges of Robot Reinforcement Learning](#rrlchallenges)
  - [Curse of dimensionality](#curse)

<div id='rluses'/>
### How's Reinforcement Learning being used today?

A variety of different problems can be solved using Reinforcement Learning. Because RL agents can learn without expert supervision, the type of problems that are best suited to RL are complex problems where there appears to be no obvious or easily programmable solution. Two of the most popular ones are:

- **Game playing** - determining the best move to make in a game often depends on a number of different factors, hence the number of possible states that can exist in a particular game is usually very large. To cover this many states using a standard rule based approach would mean specifying an also large number of hard coded rules. RL cuts out the need to manually specify rules, agents learn simply by playing the game. For two player games such as backgammon, agents can be trained by playing against other human players or even other RL agents.

- **Control problems** - such as elevator scheduling. Again, it is not obvious what strategies would provide the best, most timely elevator service. For control problems such as this, RL agents can be left to learn in a simulated environment and eventually they will come up with good controlling policies. Some advantages of using RL for control problems is that an agent can be retrained easily to adapt to environment changes, and trained continuously while the system is online, improving performance all the time.

A nice and relatively recent (at the time of writing) example of reinforcement learning is presented at DeepMind's paper [Human-level control through deep reinforcement learning](https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf).

<div id='rlrobotics'/>
### Why is Reinforcement Learning relevant for robots?

As J. Kober, J. Andrew (Drew) Bagnell, and J. Peters point out in *Reinforcement Learning in Robotics: A Survey*:

>Reinforcement learning offers to robotics a framework
and set of tools for the design of sophisticated
and hard-to-engineer behaviors.

which tend to be most of the existing ones in the real world. As a rule of thumb, someone can probably think that those tasks that are complex for a human could probably be easily done by a robot while things that we humans do easily and with little effort, tend to be highly complex for a robot. This leads many to think that robots excel at solving complexity but perform poorly on trivial tasks (from a human perspective). In other words:

>What's complex for a human, robots can do easily and viceversa - VÃ­ctor Mayoral Vilches

Let's take a simple example to illustrate this claim. Image we have a robot manipulator with three joints on top of a table repeatedly performing some task. Traditionally, robotic engineers in charge of the specific task will either design the whole application or use existing tools (produced by the manufacturer) to *program* such application. Regardless of the tools and complexity level, at some point someone should've derived its inverse kinematics accounting for possible errors within each motor/joint, included sensing to create a closed loop that enhances the accuracy of the model, designed the overall control scheme, programmed *calibration routines*, ... all to get a robot that produces deterministic movements in a **controlled environement**.

But the fact is that **(real) environments are not controllable** and robots nowadays pay the bill (someone that has ever produced robot demonstrations should know what i mean).

>Problems in robotics are often best represented with
high-dimensional, continuous states and actions (note
that the 10-30 dimensional continuous actions common
in robot reinforcement learning are considered large
(Powell, 2012)). In robotics, it is often unrealistic to
assume that the true state is completely observable
and noise-free. 

Coming back to J. Kober et al.'s paper:

>Reinforcement learning (RL) enables a robot to
autonomously discover an optimal behavior through
trial-and-error interactions with its environment. Instead
of explicitly detailing the solution to a problem,
in reinforcement learning the designer of a control task
provides feedback in terms of a scalar objective function
that measures the one-step performance of the
robot.

Which actually makes a lot of sense!. Think of how we learn about speficic tasks. Let's take me shooting a basketball:

1. I get myself behind the 3 point line and get ready for a shot (it's relevant to note it should be behind the 3 point line, otherwise i would totally make it every time)

2. At this point, my consciousness has no whatsoever information about the exact distance to the basket, neither the strength I should use to use to make the shot so my brain produces *an estimate* based on the model that I have (built upon years of trial an error shots).

3. With this estimate, I produce a shot. Let's assume I miss the shot, which I notice through my eyes (sensors). Again, the information perceived through my eyes is not accurate thereby what I pass to my brain is not: "*I missed the shot by 5.45 cm to the right*" but more like "*The shot was slightly too much to the right and i missed*". This information updates the model in my brain which receives a *negative reward*.
We could get ourselves discussing about why did my estimate failed. Was it because the model is wrong regardless of the fact that I've made hundreds of 3-pointers before with that exact model? Was it because of the wind (playing outdoors generally)? or was it because i didn't eat properly that morning?. It could easily be all of those or none, but the fact is that many of those aspects can't really be controlled be me. So i proceed *iterating*:

4. With the updated model, i make another shot which in case it fails drives me to step 2) but if I make it, I proceed to step 5).

5. Making a shot means that my model did a good job so my brain strengthens those links that produced a proper shot by giving them a *positive reward*.

>For robotics to grow exponentially, we need to be able to tackle complex problems and the trial-error approach that RL represents seems the right way.


<div id='rrltecniques'/>
### Robot Reinforcement Learning

The goal of reinforcement learning is to find a mapping from states `x` to actions,
called policy \\( \pi \\), that picks actions `a` in given states
`s` maximizing the cumulative expected reward `r`.

To do so, reinforcement learning discovers an optimal policy \\( \pi* \\) that maps states (or observations) to actions so as to maximize the expected return J, which corresponds to:

$$J_\pi = E \bigg[ R(\tau | \pi) \bigg] = \int R(\tau) p_\pi (\tau) d\tau $$

where \\( p_\pi (\tau) \\) is the distribution over the trajectory \\( \tau = (x_0, a_0, x_1, a_1, ...) \\) and \\( R(\tau) \\) is the accumulated reward in the trajectory defined as:

$$ R(\tau) = \sum_{t=0}^\infty \gamma^t r(x_t, a_t) $$

being \\( \gamma_t \in [0, 1) \\) the discount factor that discounts rewards further in
the future. With this general mathematical setup, many tasks in robotics can be naturally formulated as reinforcement learning (RL) problems. 

Traditional methods in RL, typically try to estimate the expected long-term reward of a policy for each state `x` and time step `t`, also called the **value function** \\( V_t^\pi (x) \\). Value function methods are sometimes called called *critic-only methods*. The idea of a critic is to first observe and estimate the performance of choosing controls on the system (i.e., the value function), then derive a policy based on the gained knowledge. In contrast, **policy search** methods which directly try to deduce the optimal policy \\( \pi* \\) are sometimes called *actor-only* methods.

<div id='function'/>
#### Function Approximation
Function approximation is a family of mathematical and statistical techniques used to represent a function of interest when it is computationally or information theoretically intractable to represent the function exactly or explicitly (e.g. in tabular
form).

According to J. Kober et al.:

>Typically, in reinforcement learning the function approximation is based on sample data collected during interaction with the environment. Function approximation is critical in nearly every RL problem, and becomes inevitable in continuous state ones. In large discrete spaces it is also often impractical to visit or even represent all states and actions, and function approximation in this setting can be used as a means to generalize to neighboring states and actions. Function approximation can be employed to represent policies, value functions, and forward models.


<div id='rrlchallenges'/>
### Challenges of Robot Reinforcement Learning (WIP)

<div id='curse'/>
#### Curse of dimensionality 

Bellman coined the term âCurse of Dimensionalityâ in 1957 when he explored optimal control in discrete high-dimensional spaces and faced an exponential explosion of states and actions. <span class="highlight">As the number of dimensions grows, exponentially more data and computation are needed to cover the complete state-action space</span>.

For example, let's take a 7 degree-of-freedom robot arm, a representation of the robotâs state would consist of its joint angles and velocities for each of its seven degrees of freedom as well as the Cartesian position and velocity of end efector. That accounts for \\(2 Ã (7 + 3) = 20 \\) states and `7-dimensional` continuous actions.

If we assume that each dimension of the state-space is discretized into ten levels, we have
10 states for a one-dimensional state-space. In our robot arm example we'll have \\( 10^{20} \\) unique states.


### Resources
- Chris Watkins, Learning from Delayed Rewards, Cambridge, 1989 ([thesis](http://www.cs.rhul.ac.uk/home/chrisw/new_thesis.pdf))
- Awesome Reinforcement Learning repository, [https://github.com/aikorea/awesome-rl](https://github.com/aikorea/awesome-rl)
- J. Kober, J. Andrew (Drew) Bagnell, and J. Peters, "Reinforcement Learning in Robotics: A Survey," International Journal of Robotics Research, July, 2013. ([pdf](http://www.ias.tu-darmstadt.de/uploads/Publications/Kober_IJRR_2013.pdf))
- Marc Peter Deisenroth, Gerhard Neumann and Jan Peters, *A Survey on Policy Search for Robotics* ([pdf](https://spiral.imperial.ac.uk/bitstream/10044/1/12051/4/2300000021-Deisenroth-Vol2-ROB-021_published.pdf))